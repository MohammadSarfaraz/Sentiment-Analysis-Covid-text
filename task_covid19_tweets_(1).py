# -*- coding: utf-8 -*-
"""task_covid19_tweets (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p9x42_IIhHZjGZcoik4hFuIX3OAXW4R6
"""

# COVID19 Tweets: Tweets with the hashtag "#covid19"

"""### Objective:
* To perform analysis on people tweets about COVID-19. Use this data to derive breakthrough insights like finding what kind of subjects use this hashtag, look at the geographical distribution (country), cluster and evaluate sentiments, look at trends (on an average, candidate shares at least 7 substantial insights).

### Loading All necessary file to build a model
"""

# Common lib necessary for performing textual data
import pandas as pd
import numpy as np 
import re
import string
import nltk
nltk.download('stopwords')

"""* Download package stopwords from nltk libraries and for visualization importing matplotlib,seaborn and wordcloud lib etc."""

# Loading some nltk module.
from nltk.corpus import stopwords
from nltk.util import ngrams

# Stop word of English alpahbet lib
stop=set(stopwords.words('english'))

from collections import defaultdict
from collections import Counter

# Necessary file for Data Visualization
import matplotlib.pyplot as plt 
import seaborn as sns
import matplotlib
import matplotlib.patches as mpatches
plt.style.use('ggplot')
from wordcloud import WordCloud

#Text Feature extraction and fit into some matrix formate
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
from sklearn.base import TransformerMixin


# decomposition of data using TruncatedSVD
from sklearn.decomposition import TruncatedSVD

# ML Classifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier
from sklearn.model_selection import train_test_split

# DL Tensorflow->keras model
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras import layers

# Atlast Accuracy score, confusion matrix, Classification report for all '0' and "1" prediction.
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report

"""### Reading CSV data using pandas lib pd.read_csv('name of file')"""

df =pd.read_csv('/content/covid19_tweets.csv')

# Top five rows of df
df.head()

df.shape

"""* Our dataset contain Rows=179108 and columns =13

### Data Description
"""

# Description of numeric dataset like no of user friends , user followers, favourites
df.describe()

# Columns Text of rows 179091 .
df['text'][179091]

"""* In Our dataset column text  contains hashtag , &amp, link,@ and some text"""

# Description of hashtag columns
df['hashtags'].describe()

"""* Detail Description about hashtag like No of unique rows i.e 52640,total rows and   most of them rows are "['COVID19']"

### Data Information
"""

# Information of Dataset like finding all columns if null rows exisists or not and there Datatype 
df.info()

"""* columns name "user_location","user_description","hashtags","source" contains some null values/rows i.e NaN and rest are no null values

### Top 10 rows of "tweet" which is text columns in df
"""

# All text rows from 0 t0 10 rows with max columns width flag
pd.options.display.max_colwidth = -1
df['text'][:10]

"""### Data Filtering using regex re to find -:
* column name 'text' contains or using hastag of covid19 i.e. #COVID19 
* ignoring uppercase,lowercase or other hashtags
* creating new dataframe name covid_df storing rows that contains hashtag of covid19 i.e. #covid19
"""

covid_df = df.loc[df['text'].str.contains('#COVID19',flags=re.I,regex=True)]

"""### Description of dataframe  covid_df"""

covid_df.describe()

"""#### Creating new Columns name 'covid_tweets' which define the user tweet -:
* If text columns consists hashtag "#covid19" irrespective of small letters or capital or any other tags mention in there tweets .
* Columns name text contains string with hashtag covid19.
* Using regex "re" to use for equal and ignore case irrespective of small or capital letters. 
* If "#covid19" exists in text means text/tweet for a perticular user tweets are related to covid19,so it is defined as True or 1.
* Otherwise tweets are related to some other information represent as "0".
* Later this  column name  'covid_tweet' became the targt variable .
"""

df.loc[df['text'].str.contains('#COVID19',flags=re.I,regex=True),'covid_tweets']=1

#Top five rows of datafarme
df['covid_tweets'].head()

"""#### Fill all null values as "0" using df['col_name'].fillna(0) method"""

# All null values in covidtweets represent as 0
df['covid_tweets']=df['covid_tweets'].fillna(0)

# Now we can see all values in covid_tweets represent as 0 for other tweets and 1 for covid19 tweets
df['covid_tweets'].head()

# Top five values of dataframe
df.loc[df['covid_tweets']==1].head()

"""#### Description of "user_location"
* Top most users_location are from india, 
* 26920  rows are unique,some are null values while some are same locations
"""

df['user_location'].describe()

"""#### Find no of times perticular location occurs in the dataset
* Grouping of user_location in count columns which represent of a perticular location occur in our dataset

* here represent top 10 values of grouping  "user_location"
"""

df['count']=1
df.groupby(df['user_location']).count()['count'].head(10)

"""### Data Visualization

* "1" represent covid tweets
* while "0" represent other tweets
* take out all no of rows exists in both '0' and '1' using df[].shape[0]
"""

# extracting the number of examples of each class
tweet_covid = df[df['covid_tweets'] == 1].shape[0]
Not_covid = df[df['covid_tweets'] == 0].shape[0]

# bar plot of the 3 classes
plt.rcParams['figure.figsize'] = (7, 5)
plt.bar(10,tweet_covid,3, label="Covid Tweet", color='blue')
plt.bar(15,Not_covid,3, label="Non covid Tweet", color='red')
plt.legend()
plt.ylabel('Number of examples')
plt.title('Propertion of examples')
plt.show()

"""#### Number of characters in tweets"""

def length(text):    
    '''a function which returns the length of text'''
    return len(text)

df['length'] = df['text'].apply(length)

plt.rcParams['figure.figsize'] = (18.0, 6.0)# figure size
bins = 150
# Histogram no of characters rep. by both the class
plt.hist(df[df['covid_tweets'] == 0]['length'], alpha = 0.6, bins=bins, label='Not covid tweets')
plt.hist(df[df['covid_tweets'] == 1]['length'], alpha = 0.8, bins=bins, label='Covid tweets')

#labelling x and y axis.
plt.xlabel('length')
plt.ylabel('numbers')
plt.legend(loc='upper right')
plt.xlim(0,150)
plt.grid()
plt.show()

fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))
tweet_len=df[df['covid_tweets']==1]['text'].str.len()
ax1.hist(tweet_len,color='blue')
ax1.set_title('Covid tweets')
tweet_len=df[df['covid_tweets']==0]['text'].str.len()
ax2.hist(tweet_len,color='red')
ax2.set_title('Other tweets')
fig.suptitle('Characters in tweets')
plt.show()

"""* Distribution of covid tweets are greater than other tweets in dataset

#### Total words consists of tweets
"""

fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))# representation of diag with a perticular size mention in figsize.
# Splitting text of class 1 with using lambda func to find length of words and map into the var
tweet_len=df[df['covid_tweets']==1]['text'].str.split().map(lambda x: len(x))
ax1.hist(tweet_len,color='blue')# histogram of class 1 represent with blue color
ax1.set_title('Covid tweets')

# Splitting text of class 0 with using lambda func to find length of words
tweet_len=df[df['covid_tweets']==0]['text'].str.split().map(lambda x: len(x))
ax2.hist(tweet_len,color='red')# class 0 represent with blue color
ax2.set_title('Not Covid tweets')
fig.suptitle('Words in a tweet')
plt.show()

"""#### Average word length in a tweet"""

fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))
word=df[df['covid_tweets']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])
sns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='blue')
ax1.set_title('Covid')
word=df[df['covid_tweets']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])
sns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='red')
ax2.set_title('Not Covid')
fig.suptitle('Average word length in each tweet')

"""#### Creating function consists of corpus list which spliting word from text/tweet which have a target later represent  as 0 and 1.
* Creating empty list name corpus and append in later or return value by calling  function
"""

def create_corpus(target):
    corpus=[]
    
    for x in df[df['covid_tweets']==target]['text'].str.split():
        for i in x:
            corpus.append(i)
    return corpus

def create_corpus_df(tweet, target):
    corpus=[]
    
    for x in df[df['covid_tweets']==target]['text'].str.split():
        for i in x:
            corpus.append(i)
    return corpus

"""#### Common stopwords in tweets"""

corpus=create_corpus(0)

dic=defaultdict(int)
for word in corpus:
    if word in stop:
        dic[word]+=1
        
top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]

# displaying the stopwords
np.array(stop)

"""#### First we will analyze tweets with class 0."""

plt.rcParams['figure.figsize'] = (18.0, 6.0)
x,y=zip(*top)
plt.bar(x,y)

"""#### Now we will analyze tweets with class 1."""

corpus=create_corpus(1) # calling method previosly define and passing value 1

#Creating dict and counting no of perticular stop words comes in text tweet
dic=defaultdict(int)
for word in corpus:
    if word in stop:
        dic[word]+=1

top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] 
    

plt.rcParams['figure.figsize'] = (18.0, 6.0)
x,y=zip(*top)
plt.bar(x,y)

"""#### Analysing punctuations
* Analyse for class 1
"""

plt.figure(figsize=(16,5))# Represent the size of the image
corpus=create_corpus(1)# Calling method with passing 1 value represent class 1

dic=defaultdict(int)
special = string.punctuation# String  of punctuation 
for i in (corpus):
    if i in special:
        dic[i]+=1
        
x,y=zip(*dic.items())
plt.bar(x,y)

"""* Now analyse punctuation in class 0 
* represent with color green
"""

plt.figure(figsize=(16,5))
corpus=create_corpus(0)
dic=defaultdict(int)
special = string.punctuation
for i in (corpus):
    if i in special:
        dic[i]+=1
        
x,y=zip(*dic.items())
plt.bar(x,y,color='green')

"""#### Common words"""

plt.figure(figsize=(16,5))
counter=Counter(corpus)
most=counter.most_common()
x=[]
y=[]
for word,count in most[:40]:
    if (word not in stop) :
        x.append(word)
        y.append(count)

sns.barplot(x=y,y=x)

"""#### N-gram analysis"""

def get_top_tweet_bigrams(corpus, n=None):
    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]

plt.figure(figsize=(16,5))
top_tweet_bigrams=get_top_tweet_bigrams(df['text'])[:10]
x,y=map(list,zip(*top_tweet_bigrams))
sns.barplot(x=y,y=x)

"""### Word Cloud

* Word Cloud for Covid Tweets
"""

## Covid_tweets
corpus_new1=create_corpus_df(df['text'],1)

# Generating the wordcloud with the values under the category dataframe
plt.figure(figsize=(15,10))
word_cloud = WordCloud(
                          background_color='black',
                          max_font_size = 80
                         ).generate(" ".join(corpus_new1[:80]))
plt.imshow(word_cloud)
plt.axis('off')
plt.show()

corpus_new0=create_corpus_df(df['text'],0)
corpus_new0[:10]

"""### Word Cloud  for Not a Covid tweets"""

# Generating the wordcloud with the values under the category dataframe
plt.figure(figsize=(15,10))
word_cloud = WordCloud(
                          background_color='black',
                          max_font_size = 80
                         ).generate(" ".join(corpus_new0[:80]))
plt.imshow(word_cloud)
plt.axis('off')
plt.show()

"""## Pre-processing
* Text cleaning
* Tweets contain different kind of noise that can harm machine learning algorithms performance. We need tocarefully get rid of them. To this particular task we will take advantage of regular expressions.

#### Rows and olumns in dataset
"""

df.shape

"""#### Cleaning  dataset text/tweets using regular expression re and tokenize into word ."""

token_text=[]
for i in range(179108):
    text = re.sub(r'@\w+', '',str(df['text'][i]))
    
    #URL’s
    text = re.sub(r'http.?://[^\s]+[\s]?', '', text)
    
    #Symbols and digits: points, hashtags, commas and all kind of symbols as well as numbers are removed.
    text = re.sub('[^a-zA-Z\s]', '', text)

    #Extra white spaces: after applying previous steps, texts could end up with extra white spaces that later the tokenizer will split as words. We remove them.
    text = re.sub("\s+", ' ', text)
    text = text.lstrip()
    text = text.rstrip()
    
    #Lowercase: we do not consider letter case so all texts are transformed to lowercase.
    text = text.lower()


    #Fixing words: people make typos (‘cafire’ ), use abbreviations (‘ppl’), acronyms (‘asap’) and 
    #different words have the same meaning (‘iphone’ and ‘phone’). 
    #These are just four examples of words that need to be fixed. 
    #This examples were found by inspecting the frequency vocabulary. 
    text = re.sub(r'\bppl\b', 'people', text)
    text = re.sub(r'\basap\b', 'as soon as possible', text)
    re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')
    def tokenize(s): 
        return re_tok.sub(r' \1 ', s).split()
    token_text.append(tokenize(text))
    #print(tokenize(text))

"""#### Creating dataframe of text token"""

tokenize = pd.DataFrame({"token":token_text})
my_tokenize_df = pd.concat([df['text'],tokenize],axis=1)# concatenate tokenize data with text

del tokenize

"""#### Compare diff bet normal tweet and tokenize tweet"""

my_tokenize_df.head()

"""#### Custom transform and cleaning data"""

# Custom transformer
class predictors(TransformerMixin):
    def transform(self, X, **transform_params):
        # Cleaning Text
        return [clean_text(text) for text in X]

    def fit(self, X, y=None, **fit_params):
        return self

    def get_params(self, deep=True):
        return {}

# Basic function to clean the text
def clean_text(text):
    # Removing spaces and converting text into lowercase
    return str(text).strip().lower()

token_pred=predictors().transform(my_tokenize_df['token'])

"""#### Using Count Vectorizer- to transform our text data to matrix formate"""

def cv(data):
    count_vectorizer = CountVectorizer()

    emb = count_vectorizer.fit_transform(data)

    return emb, count_vectorizer

list_corpus = token_pred
list_labels = df["covid_tweets"].tolist()

# Splitting into training and testing
X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2, 
                                                                                random_state=40)

X_train_counts, count_vectorizer = cv(X_train)
X_test_counts = count_vectorizer.transform(X_test)

"""#### Plot LSA Visualization"""

def plot_LSA(test_data, test_labels, savepath="PCA_demo.csv", plot=True):
        lsa = TruncatedSVD(n_components=2)
        lsa.fit(test_data)
        lsa_scores = lsa.transform(test_data)
        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}
        color_column = [color_mapper[label] for label in test_labels]
        colors = ['orange','blue']
        if plot:
            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))
            orange_patch = mpatches.Patch(color='orange', label='Not Covid')
            blue_patch = mpatches.Patch(color='blue', label='Covid')
            plt.legend(handles=[orange_patch, blue_patch], prop={'size': 30})

fig = plt.figure(figsize=(16, 16))          
plot_LSA(X_train_counts, y_train)
plt.show()

"""#### Plot LSA using tfidfVectorizer"""

def tfidf(data):
    tfidf_vectorizer = TfidfVectorizer()

    train = tfidf_vectorizer.fit_transform(data)

    return train, tfidf_vectorizer

X_train_tfidf, tfidf_vectorizer = tfidf(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

"""plot lsa for tfidf"""

fig = plt.figure(figsize=(16, 16))          
plot_LSA(X_train_tfidf, y_train)
plt.show()

"""### Training our model using Other Classifier
* Fitting Machine learning algo and compare there training and testing score
* To check Data underfitting or Overfitting the model by comparing there score.
* In underfitting condition- model accuracy is very low.
* In Overfitting condtion- training accuracy is high but testing accuracy is exponentially low as compare to training.
* Chose the best algo suited for our model
* Find Accuracy score , confusion matrix, classsification report
"""

lr = LogisticRegression()
rfc =RandomForestClassifier(max_depth=30)
lsvc = LinearSVC(C=0.01)
ad_boost = AdaBoostClassifier()

ml_model = []
ml_model.append(("LogisticRegression",lr))
ml_model.append(('RandomForestClassifier',rfc))
ml_model.append(('LinearSVC',lsvc))

for name, algo in ml_model:
    algo.fit(X_train_tfidf,y_train)
    train_score=algo.score(X_train_tfidf,y_train)
    test_score = algo.score(X_test_tfidf,y_test)
    msg = "%s = (training score): %f (testing score:) %f"%(name,train_score,test_score)
    print(msg)

"""* Logistic reg will predict the best score niether underfitting nor overfitting as compare to other classifier"""

# Prediction
from sklearn import metrics
lr.fit(X_train_tfidf,y_train)
pred = lr.predict(X_test_tfidf)

print("LogReg  Classification_report:",metrics.classification_report(y_test,pred))
print("====================================================================================\n")
print("LogReg Confusion Matrix:",metrics.confusion_matrix(y_test, pred))
print("====================================================================================\n")
print("Accuracy",metrics.accuracy_score(y_test,pred))

### Deep Learning LSTM using keras model

"""* No of rows and columns in train dataset"""

X_test_tfidf.shape

"""* Next, we'll convert text data into token vectors."""

tokenizer = Tokenizer(num_words=100)
tokenizer.fit_on_texts(token_pred)
xtrain= tokenizer.texts_to_sequences(X_train)
xtest= tokenizer.texts_to_sequences(X_test)

"""* Convert all training and test data into numpy array"""

X_test = np.array(X_test)
X_train = np.array(X_train)
y_train = np.array(y_train)
y_test = np.array(y_test)

"""* We'll apply a padding method to add zeros and set the fixed size into each vector."""

maxlen=10
xtrain=pad_sequences(xtrain,padding='post', maxlen=maxlen)
xtest=pad_sequences(xtest,padding='post', maxlen=maxlen)

print(X_train[3])

print(xtrain[3])

"""#### Defining the LSTM model

* We apply the Embedding layer for input data before adding the LSTM layer into the Keras sequential model. The model definition goes as a following.
"""

embedding_dim=50
model=Sequential()
model.add(layers.Embedding(input_dim=100,
      output_dim=embedding_dim,
      input_length=maxlen))
model.add(layers.LSTM(units=20,return_sequences=True))
model.add(layers.LSTM(units=10))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(28))
model.add(layers.Dense(1, activation="sigmoid"))
model.compile(optimizer="adam", loss="binary_crossentropy", 
     metrics=['accuracy'])
model.summary()

"""#### Finally, we'll train the model and check the training accuracy."""

history = model.fit(xtrain,y_train, epochs=20, batch_size=64)

"""#### Visualize the training loss and te validation accuracy to see if the model is overfitting"""

pd.DataFrame(history.history).plot()
plt.grid(True)
plt.gca().set_ylim()
plt.show()

loss, acc = model.evaluate(xtrain, y_train,verbose=False)
print("Training Accuracy: ", acc)

loss, acc = model.evaluate(xtest, y_test, verbose=False)
print("Test Accuracy: ", acc)
print("Test Loss: ", loss)

"""#### Predicting test data"""

ypred=model.predict(xtest)

ypred[ypred>0.5]=1 
ypred[ypred<=0.5]=0

"""#### Confusion Matrix define-:
* True Negative
* False Positive
* False Negative
* True Positive
  * diagonal element are correct prediction rest are incorrect prediction
  * Accuracy =Sum of all correct prediction(diagonal data) divided by sum of all the data(correct or wrong prediction data)
"""

cm = confusion_matrix(y_test, ypred)
print(cm)

"""#### Classification report Define_:
* precision-:precision is the fraction of relevant instances among the retrieved instances
* recall:-while recall is the fraction of the total amount of relevant instances that were actually retrieved
* f1-score-:F1 is an overall measure of a model’s accuracy that combines precision and recall, in that weird way that addition and multiplication just mix two ingredients to make a separate dish altogether
"""

cr = classification_report(y_test,ypred)
print(cr)

"""### Finally result or prediction of test data  store in "re_store" list."""

result=zip(X_test, y_test, ypred)
res_store=[]
for i in result:
    res_store.append(i)

"""### Top five values of result data with classes represent 1 or 0"""

res_store[:5]